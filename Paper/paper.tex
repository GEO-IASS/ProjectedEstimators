\documentclass{article}

\usepackage{times}
\usepackage{graphicx}
\usepackage{subfigure} 

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{hyperref}
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage{icml2016} 

\usepackage{amsmath}
\usepackage{amssymb}
\newcommand{\Xe}{\vec{X}_\mathrm{e}  }
\newcommand{\XeT}{\vec{X}_\mathrm{e}^\top}

\newcommand{\ye}{\begin{bmatrix} \vec{y}  \\ \vec{y}_\mathrm{u} \end{bmatrix}}
\newcommand{\G}{\left(\Xe^\top \Xe \right)^{-1}}
\newcommand{\missidentity}{\begin{bmatrix} 0 & 0 \\ 0 & \textbf{I }\end{bmatrix}}
\newcommand{\Greg}{\left(\Xe^T \Xe + \lambda \missidentity \right)^{-1}}
\newcommand{\Cb}{\mathcal{C}_{\beta}}
\newtheorem{theorem}{Theorem}
\renewcommand{\vec}[1]{\mathbf{#1}}
\DeclareMathOperator*{\argmin}{arg\,min}

\icmltitlerunning{Projected Estimators for Robust Semi-supervised Classification}

\begin{document}

\twocolumn[
\icmltitle{Projected Estimators for Robust Semi-supervised Classification}

\icmlauthor{Jesse H. Krijthe}{jkrijthe@gmail.com}
\icmladdress{Delft University of Technology,
            Mekelweg 4, 2628CD, Delft, The Netherlands}
\icmlauthor{Marco Loog}{m.loog@tudelft.nl}
\icmladdress{Delft University of Technology,
            Mekelweg 4, 2628CD, Delft, The Netherlands}

\icmlkeywords{Least Squares Classifier, Semi-supervised Learning, Projection}

\vskip 0.3in
]
 
\begin{abstract} 
For semi-supervised techniques to be applied safely in practice we at least want methods to outperform their supervised counterparts. We study this question for classification for the well-known quadratic loss function. Using a projection of the supervised estimate onto a set of constraints imposed by the unlabeled data, we find we can safely improve over the supervised solution in terms of this quadratic loss. Unlike other approaches to semi-supervised learning, the procedure does not rely on additional assumptions that are not intrinsic to the classifier at hand. It is theoretically demonstrated that, measured on the labeled and unlabeled training data, this semi-supervised procedure never gives a lower quadratic loss than the supervised alternative.  To our knowledge this is the first approach that offers such strong, albeit conservative, guarantees for improvement over the supervised solution. The characteristics of our approach are explicated using benchmark datasets to further understand the similarities and differences between the quadratic loss criterion used in the theoretical results and the classification accuracy often considered in practice.
\end{abstract} 
 
\section{Introduction}
\label{Introduction}
We consider the problem of semi-supervised classification using the quadratic loss function, which is also known as least squares classification or Fisher's linear discriminant classification \citep{Hastie2001,Poggio2003}. Suppose we are given an $N_l \times d$ matrix with feature vectors $\vec{X}$, labels $\vec{y} \in \{0,1\}^{N_l}$ and an $N_u \times d$  matrix with unlabeled objects $\vec{X_u}$ from the same distribution as the labeled objects. The goal of semi-supervised learning is to improve the classification decision function $f: \mathbb{R}^d \to \mathbb{R}$ using the unlabeled information in $\vec{X_u}$ as compared to the case where we do not have these unlabeled objects. In this work, we focus on linear classifiers where $f(\vec{x})=\vec{w}^T \vec{x}$. 

Much work has been done on semi-supervised classification, in particular on what additional assumptions about the unlabeled data may help improve classification performance. These additional assumptions, while successful in some settings, are less successful in others, when they do not hold. In effect they can greatly deteriorate performance when compared to a supervised alternative \citep{Cozman2006}. Since, in semi-supervised applications, the number of labeled objects may be small, the effect of these assumptions is often untestable. 

In this work, we avoid additional assumptions altogether to study what improvements are still possible by using the unlabeled data. Given the choice of the quadratic loss as a surrogate loss function, we introduce a constrained set of parameter vectors induced by the unlabeled data, which does not rely on additional assumptions about the data. Using a projection of the supervised solution onto this constrained set, we derive a method that can be proven to never degrade the surrogate loss under consideration when compared to the supervised solution in the transductive setting. Experimental results indicate that it not only never degrades, but often improves performance. Thhis transductive setting involves testing the performance of a procedure on the labeled and unlabeled training objects. Our experiments also indicate the results in the transductive setting transfer to the inductive setting where performance is evaluated on objects in a test set that were not used as unlabeled objects during training.

Others have attempted to mitigate the problem of reduction in performance in semi-supervised learning by introducing safe versions of semi-supervised learners \citep{Li2011,Loog2010,Loog2014a}. These procedures do not offer any guarantees or only do so once particular assumptions about the data hold.
While the method proposed here offers a very conservative procedure that might not offer much improvement over the supervised classifier, it is the first procedure for which it is possible to give strong guarantees of non-degradation of the type offered in Theorem~\ref{thm:robustness}, without additional assumptions about the data. 

% This constitutes a novel result in semi-supervised learning about the improvement over the supervised alternative disregarding the unlabeled data. This non-degradation property is important in practical applications, since one would like to be sure that the effort of the collection of, and computation with unlabeled data does not have an adverse effect. Our work is a conceptual step towards such methods. The main goal of this work is to prove and illustrate this property.

The rest of this work is structured as follows. The next section discusses related work. Section \ref{section:projections} introduces our projection approach to semi-supervised learning. Section \ref{section:theory} gives the theoretical performance guarantee and its implications. Section \ref{section:interpretations} provides some alternative interpretations for the proposed method. In Section \ref{section:empirical} empirical illustrations on benchmark datasets are presented to understand how the theoretical results in terms of quadratic loss in Section 4 relate to classification error. We end with a discussion of the results and conclude.

\section{Prior Work and Assumptions}

Early work on semi-supervised learning treated it as a missing data problem, through the use of Expectation Maximization or closely related self-learning \citep{McLachlan1975}. Self-learning is a simple wrapper method around any supervised procedure. Starting with a supervised learner trained only on the labeled objects, we predict labels for the unlabeled objects. Using the known labels and the predicted labels for the unlabeled objects, or potentially the predicted labels with highest confidence, we can retrain the supervised learner. This process is iterated until the predicted labels converge. Although simple, this procedure has seen some practical success \citep{Nigam2000}.

More recent work on semi-supervised methods involves either the assumption that the decision boundary is in a low-density region of the feature space, or that the data is concentrated on a low-dimensional manifold. A well known procedure using the first assumption is the Transductive SVM \citep{Joachims1999}. It can be interpreted as minimizing the following objective:
\begin{multline}
\label{eq:TSVM}
\min_{\vec{w} \in \mathbb{R}^d,\vec{y}_\text{u} \in \{-1,+1\}^{N_u}} \sum_{i=1}^{N_l} \max(1-\vec{y}_i \vec{w}^\top \vec{x},0) + \lambda ||\vec{w}||^2 \\ + \nu \sum_{i=1}^{N_u} \max(1-\vec{y}_\text{u}^{(i)} \vec{w}^\top \vec{x},0)
\end{multline}
where class labels are encoded using $+1$ and $-1$. This leads to a hard to optimize, non-convex, problem, due to the dependence on the labels of the unlabeled objects $\vec{y}_\text{u}$. Others, such as \citep{Sindhwani2006}, have proposed procedures to efficiently find a good local minimum of this function. Similar low-density ideas have been proposed for other classifiers, such as entropy regularization for logistic regression \citep{Grandvalet2005} and a method for Gaussian processes \citep{Lawrence2004}. One challenge with these procedure is setting the additional parameter $\nu$ that is introduced to control the effect of the unlabeled objects. This is both a computational problem, since minimizing \eqref{eq:TSVM} is already hard for a single choice of $\nu$, as well as a estimation problem. If the parameter is incorrectly set using, for example, cross-validation on a limited set of labeled examples, the procedure may actually reduce performance as compared to a regular SVM which disregards the unlabeled data. It is this behavior that our procedure avoids. While it may be outperformed by the TSVM if the low-density assumption holds, robustness against deterioration would still constitute an important property in the cases when we are not sure whether it holds.

An attempt at safety in semi-supervised learning was introduced in \citep{Li2011}, who propose a safe variant for semi-supervised support vector machines. By constructing a set of possible decision boundaries using the unlabeled and labeled data, the decision boundary is chosen that is least likely to degrade performance. The goal of our work is also similar to that of \citep{Loog2010,Loog2014a}, who introduce a semi-supervised version of linear discriminant analysis, which is closely related to the least squares classifier considered here. There, explicit constraints are proposed that take into account the unlabeled data. In our work, these constraints need not be explicitly derived, but follow  directly from the choice of loss function and the data. While the impetus for these works is similar to ours, they provide no theory to guarantee no degradation in performance will occur for any measure of performance, similar to our results in Section \ref{TheoreticalResults}.

\section{Projection Method}
\label{section:projections}
We consider classification using a quadratic surrogate loss \citep{Hastie2001}. In the supervised setting, the following objective is minimized for $\vec{w}$:
\begin{equation}
\label{eq:supervisedloss}
L(\vec{w},\vec{X},\vec{y}) = \lVert \vec{X} \vec{w} - \vec{y} \rVert^2
\end{equation}
The supervised solution $\vec{w}_{\text{sup}}$ is given by the minimization of \eqref{eq:supervisedloss} for $\vec{w}$. The well-known closed form solution to this problem is given by
\begin{equation}
\label{eq:supervisedsolution}
\vec{w}_{\text{sup}} = (\vec{X}^\top \vec{X})^{-1} \vec{X}^\top \vec{y}
\end{equation}
If the true labels corresponding to the unlabeled objects, $\vec{y}_\text{u}^{\ast}$, would be given, we could incorporate these by extending the vector of labels ${\vec{y}_\text{e}^\ast}^\top = \left[ \vec{y}^\top {\vec{y}_\text{u}^\ast}^\top \right]^\top$ as well as the design matrix $\vec{X}_\text{e}^\top = \left[ \vec{X}^\top \vec{X}_\text{u}^\top \right]$ and minimize $L(\vec{w},\Xe, \vec{y}_\text{e}^\ast)$ over the labeled as well as the unlabeled objects. We will refer to this oracle solution as $\vec{w}_\text{oracle}$. 

The motivation behind the projection method is to form a constrained set of parameter vectors $\vec{w}$, informed by the labeled \emph{and unlabeled} objects, that is guaranteed to include $\vec{w}_\text{oracle}$. We will then find the closest projection of $\vec{w}_{\text{sup}}$ onto this set, using a chosen distance measure. This new estimate, $\vec{w}_{\text{semi}}$, will then be closer to the oracle solution than the supervised solution $\vec{w}_{\text{sup}}$ in terms of this distance measure, as we will demonstrate. For a particular choice of measure, we will prove (Section \ref{TheoreticalResults})  that $\vec{w}_{\text{semi}}$ will always have lower quadratic loss when measure on the labeled and unlabeled training data, as compared to $\vec{w}_{\text{sup}}$.

To form the constrained set, consider all possible labels for the unlabeled objects $\vec{y}_\text{u} \in [0,1]^{N_u}$. This includes fractional labelings, where an objects is partly assigned to class $0$ and partly to class $1$. For instance, $0$ indicates the object is exactly equally assigned to both classes. For a particular labeling $\vec{y}_\text{e}^\top = \left[ \vec{y}^\top \vec{y}_\text{u}^\top \right]^\top$, we can find the corresponding parameter vector by minimizing $L(\vec{w},\vec{X}_\text{e},\vec{y}_\text{e})$ for $\vec{w}$.
This objective remains the same as \eqref{eq:supervisedloss} except that fractional labels are now also included. Minimizing the objective for all possible labelings generates the following set of solutions:
\begin{equation}
\label{eq:constrainedregion}
\Theta=\left\{ \G \XeT \ye \mid \vec{y}_\text{u} \in [0,1]^{N_u} \right\} \, .
\end{equation}
Note that this set, by construction, will also contain the solution $\vec{w}_\text{oracle}$, corresponding to the true but unknown labeling $\vec{y}_\text{e}^{\ast}$. Typically, $\vec{w}_\text{oracle}$ is a better solution than $\vec{w}_\text{sup}$ and so we would like to find a solution more similar to $\vec{w}_\text{oracle}$. This can be accomplished by projecting $\vec{w}_\text{sup}$ onto $\Theta$. It remains to determine how to calculate the distance between $\vec{w}_\text{sup}$ and any other $\vec{w}$ in the space. For reasons which will become clear below, we will consider the following metric:
\begin{equation}
\label{eq:metric}
d(\vec{w},\vec{w}^\prime)=\sqrt{\left( \vec{w}-\vec{w}^\prime \right)^\top \vec{X}_{\circ}^\top \vec{X}_{\circ}  \left( \vec{w}-\vec{w}^\prime \right)}
\end{equation}
where we assume $\vec{X}_{\circ}^\top \vec{X}_{\circ}$ is a positive definite matrix. The projected estimator can now be found by minimizing this distance between the supervised solution and solutions in the constrained set:
\begin{equation}
\label{eq:projection}
\vec{w}_\mathrm{semi} = \min_{\vec{w} \in \Theta} d(\vec{w},\vec{w}_\text{sup})
\end{equation}
Setting $\vec{X}_\circ=\Xe$ measures the distances using both the labeled and unlabeled data. This choice has the desirable theoretical properties leading us to the sought-after improvement guarantees as we will demonstrate in the next section.

By plugging in the closed form solution of $\vec{w}_\text{sup}$ and $\vec{w}$ for a given $\vec{y}_\text{u}$, this problem can be written as a convex minimization problem in terms of $\vec{y}_\text{u}$, the unknown, fractional labels of the unlabeled data. We are left with a quadratic programming problem, which can be solved using a simple gradient descent procedure that takes into account the constraint that the labels are within $[0,1]$. The solution of this quadratic programming problem $\vec{\hat{y}}_\text{u}$ can then be used to find  $\vec{w}_\text{semi}$ by treating these imputed labels as the true labels of the unlabeled objects and using the result in Equation \eqref{eq:supervisedsolution}.

\section{Theoretical Analysis}
\label{section:theory}

% \subsection{Robustness}
We start out by formulating our main result.
\begin{theorem}
\label{th:robustness}
Given $\vec{X}$, $\vec{X}_\mathrm{u}$ and $\vec{y}$, $\Xe^\top \Xe$ positive definite and $\vec{w}_\mathrm{sup}$ given by \eqref{eq:supervisedsolution}. For the projected estimator $\vec{w}_\mathrm{semi}$ proposed in \eqref{eq:projection}, the following result holds:
$L(\vec{w}_\mathrm{semi},\Xe,\vec{y}_\mathrm{e}^{\ast}) \leq L(\vec{w}_\mathrm{sup},\Xe,\vec{y}_\mathrm{e}^{\ast}) $
\end{theorem}
In other words: $\vec{w}_\text{semi}$ will \emph{always} be at least as good or better than $\vec{w}_\text{sup}$, in terms of the quadratic surrogate loss on all, labeled and unlabeled, training data.

The proof of this result follows from a geometric interpretation of our procedure. Consider the following inner product that induces the distance metric in Equation \eqref{eq:metric}:
\begin{equation}
\left\langle \vec{w}, \vec{w}^\prime \right\rangle = \vec{w}^\top \vec{X}_\text{e}^\top \vec{X}_\text{e} \vec{w}^\prime
\end{equation}
Let $\mathcal{H}_{\vec{X}_\text{e}} = ( \mathbb{R}^d,\left\langle ., . \right\rangle )$ be the inner product space corresponding with this inner product. As long as $\XeT \Xe$ is positive definite, this is a Hilbert space. Next, note that the constrained space $\Theta$ is convex. More precisely, because, for any $k \in [0,1]$ and $\vec{w}_\text{1},\vec{w}_\text{2} \in \Theta$ we have that
\begin{align}
(1-k) \vec{w}_\text{1} + k \vec{w}_\text{2} & = \nonumber \\
(1-k) \G \XeT \left[\vec{y}^\top \vec{y}_\text{1}^\top \right] + & \nonumber \\ 
 k \G \XeT \left[\vec{y}^\top \vec{y}_\text{2}^\top \right] & = \nonumber \\ 
\G \XeT \left[\vec{y}^\top ~~ k \vec{y}_\text{1}^\top + (1-k) \vec{y}_\text{2}^\top \right] & \in \Theta \nonumber
\end{align}

where the last equality holds because $k \vec{y}_\text{1}^\top + (1-k) \vec{y}_\text{2}^\top \in [0,1]^{N_u}$.

By construction $\vec{w}_\text{semi}$ is the closest projection of $\vec{w}_\text{sup}$ onto this convex constrained set $\Theta$ in $\mathcal{H}_{\vec{X}_\text{e}}$. One of the properties for projections onto convex subspaces in a Hilbert space is \citep[Proposition 1.4.1.]{Aubin2000} that 
\begin{equation}
\label{eq:projectiontheorem}
d(\vec{w}_\text{semi},\vec{w}) \leq d(\vec{w}_\text{sup},\vec{w})
\end{equation}
for any $\vec{w} \in \Theta$. In particular consider $\vec{w}=\vec{w}_\text{oracle}$, which by construction is within $\Theta$. That is, all possible labelings correspond to an element in $\Theta$, so this also holds for the true labeling $\vec{y}_\text{u}^\ast$. Plugging in the closed form solution of $\vec{w}_\text{oracle}$ into \eqref{eq:projectiontheorem} we find:
\begin{flalign}
d(\vec{w}_\text{semi},\vec{w}_\text{oracle})^2 = & \vec{w}_\text{semi}^\top \Xe^\top \Xe \vec{w}_\text{semi} \\ \nonumber
& - 2 \vec{w}_\text{semi}^\top \Xe^\top {\vec{y}_\text{e}^\ast} + {\vec{y}_\text{e}^\ast}^\top {\vec{y}_\text{e}^\ast}\\ \nonumber
& +  C\\ \nonumber
= & L(\vec{w}_\text{semi},\Xe,\vec{y}_\text{e}^{\ast}) + C\\ \nonumber
\end{flalign}
and
\begin{flalign}
d(\vec{w}_\text{sup},\vec{w}_\text{oracle})^2 = & \vec{w}_\text{sup}^\top \Xe^\top \Xe \vec{w}_\text{sup} \\ \nonumber
& - 2 \vec{w}_\text{sup}^\top \Xe^\top {\vec{y}_\text{e}^\ast} +  {\vec{y}_\text{e}^\ast}^\top {\vec{y}_\text{e}^\ast} \\ \nonumber
& +  C\\ \nonumber
= & L(\vec{w}_\text{sup},\Xe,\vec{y}_\text{e}^{\ast}) + C \\ \nonumber
\end{flalign}
Where $C$ is the same constant in both cases. From this the result in Theorem \ref{th:robustness} follows directly.

It is possible to derive a similar result for performance improvement on the unlabeled data alone by using $\vec{X}_\circ=\vec{X}_\text{u}$ in the distance measure and changing the constrained hypothesis space to:
\begin{equation}
\label{eq:constrainedregion2}
\Theta_\text{u} = \left\{ (\vec{X}_\mathrm{u}^\top \vec{X}_\mathrm{u})^{-1} \vec{X}_\mathrm{u}^\top \vec{y}_\mathrm{u} \mid \vec{y}_\text{u} \in [0,1]^{N_u} \right\}
\end{equation}
Which would lead to a guarantee of the form:
\begin{equation}
L(\vec{w}_\mathrm{semi},\vec{X}_\mathrm{u},\vec{y}_\mathrm{u}^{\ast}) \leq L(\vec{w}_\mathrm{sup},\vec{X}_\mathrm{u},\vec{y}_\mathrm{u}^{\ast})
\end{equation}
However, since we would not just like to perform well on the given unlabeled data, but on unseen data from the same distribution as well, we also consider the labeled data in the construction of the constrained hypothesis space.

The result in Theorem \ref{th:robustness} also holds if we include regularization in the supervised classifier. Using $L_2$ regularization, the supervised solution becomes:
\begin{equation}
\label{eq:regsupervisedsolution}
\vec{w}_{\text{sup}} = (\vec{X}^\top \vec{X} + \lambda \vec{I})^{-1} \vec{X}^\top \vec{y}
\end{equation}
where $\lambda$ is a regularization parameter and $\vec{I}$ a $d \times d$ identity matrix. Theorem \ref{th:robustness} also holds for this regularized supervised estimator.

Since the inequality in Theorem \ref{th:robustness} is not necessarily a strict inequality, it is important to get an idea when we can expect improvement of the semi-supervised learner, rather than just equality of the losses. Consider a single unlabeled object. Improvement happens whenever $\vec{w}_\text{sup} \neq \vec{w}_\text{semi}$, which occurs if $\vec{w}_\text{sup} \notin \Theta$.  For this to occur it needs to be impossible to assign labels $\text{y}_\text{u}$ such that we can retrieve the $\vec{w}_\text{sup}$ by minimizing $L(\vec{w},\vec{X}_\text{e},\vec{y}_\text{e})$. This in turn occurs when there is no $\text{y}_\text{u} \in [0,1]$ for which the gradient

\begin{equation}
\nabla \lVert \vec{X}_\text{e} \vec{w} - \vec{y}_\text{e} \rVert^2\bigg|_{\vec{w}=\vec{w}_\text{sup}}=\vec{0}.
\end{equation}

This happens only if $\vec{x}_\text{u}^\top \vec{w}_\text{sup} > 1$ or $\vec{x}_\text{u}^\top \vec{w}_\text{sup} < 0$. In other words, if observations $\vec{x}_\text{u}$ are possible with values that are sufficiently large (or small) and $\vec{w}_\text{sup}$ is not small enough to mitigate this, an update will occur. For many datasets, we might expect this to be true for at least one observation in a large set of unlabeled objects. This is especially true if the  supervised solution is not sufficiently regularized and the $\vec{x}_\text{u}^\top \vec{w}_\text{sup}$ can easily be larger than $1$ or smaller than $0$. The experiments in Section \ref{section:empirical} indeed confirm that generally improvements can be expected by means of the proposed semi-supervised learning strategy.

\section{Interpretation}
\label{section:interpretations}
The projection method in Equation \eqref{eq:projection}, using $\vec{X}_{\circ}=\Xe$ in the distance measure, can be rewritten in a different form:
\begin{equation}
\argmin_{\vec{w}_\text{semi}} \max_{\vec{y}_\text{u} \in [0,1]^{N_u}} L(\vec{w}_\text{semi},\Xe,\vec{y}_\text{e}) - L(\vec{w}_\text{sup},\Xe,\vec{y}_\text{e})
\end{equation}
In other words, the procedure can also be interpreted as a minimization of the difference in loss in the transductive setting between the new solution and the supervised solution, over all possible labelings of the data. From this perspective, it is apparent that this estimator may be conservative, since it has to have low loss for all possible labelings, even very unlikely ones.

In a similar way an alternative choice of distance function, $\vec{X}_{\circ}=\vec{X}$, has a different interpretation. It is the minimizer of the the supervised loss function under the constraint that its solution has to be a minimizer for some labeling of the unlabeled data:
\begin{equation}
\argmin_{\Theta} L(\vec{w},\vec{X},\vec{y})
\end{equation}
with $\Theta$ defined as in Equation \eqref{eq:constrainedregion}. This criterion seems less conservative since the solution does not need to have a low loss for all possible labelings, it merely has to work well on the labeled examples. For this distance measure, the proof in Section \ref{TheoreticalResults} no longer holds, but empirical results indicate it may have better performance in practice, while it still protects against deterioration in performance by minimizing the loss over only the labeled objects.

Another interpretation of the projection procedure is that it minimizes the squared difference between the predictions of the supervised solution and a new semi-supervised solution on a the set of data points in $\vec{X}_{\circ}$, while ensuring the semi-supervised solution corresponds to a possible labeling of the unlabeled objects:
\begin{equation}
\min_{\vec{w} \in \Theta} \lVert \vec{X}_{\circ} \vec{w} - \vec{X}_{\circ} \vec{w}_\text{sup} \lVert^2
\end{equation}
Since this comparison requires only the features in $\vec{X}_{\circ}$ and not the corresponding labels, this can be done either on the labeled data, when we choose $\vec{X}_{\circ}=\vec{X}$, but also on the labeled and unlabeled data combined when $\vec{X}_{\circ}=\Xe$. This interpretation is similar to the work of \cite{Schuurmans2002}, where the unlabeled objects are also used to measure the difference in predictions of two hypotheses. 

\section{Experimental Analysis}
\label{section:empirical}
For our experiments, we consider $23$ classification datasets. $6$ of these are the semi-supervised learning benchmark datasets proposed by \citep{Chapelle2006}, while the other $17$ were retrieved from the UCI Machine Learning repository \citep{Bache2013}. All of the datasets are binary classification problems, or were turned into two-class problems by merging several similar classes. As a preprocessing step, missing input values were imputed and PCA was applied on the whole dataset. The code to reproduce the results presented here is available from the first author's website.

The number of features used is either set to half the number of labeled examples or the number of labeled examples is chosen such that the $N_l>d$. This is necessarily to have a high probability that the matrix $\vec{X}_\text{e}^\top \vec{X}_\text{e}$ is positive definite, which was a requirement of Theorem \ref{th:robustness}. More importantly, this avoids peaking behaviour \citep{Raudys1998, Opper1996}, were the unregularized supervised least squares classifier has low performance when the matrix $\vec{X}^\top \vec{X}$ is not full-rank. % For the SVM and TSVM implementations we made use of the SVMlin software \citep{Sindhwani2006}.

\subsection{Robustness}
To illustrate Theorem \ref{th:robustness} experimentally, as well as study the behavior of the proposed procedure in the inductive setting, we set up the following experiment. For each of the $15$ datasets, we randomly select $100$ labeled objects. We then randomly sample, with replacement, $1000$ objects as the unlabeled objects from the dataset. In addition, a test set of $1000$ objects is also sampled with replacement. This procedure is repeated $100$ times and the differences between the average quadratic losses for the supervised and the semi-supervised procedure $L(\vec{w}_\text{sup},\Xe,\vec{y}_\text{e}^{\ast}) - L(\vec{w}_\text{semi},\Xe,\vec{y}_\text{e}^{\ast})$, which was the object of Theorem \ref{th:robustness}, is shown in Figure \ref{fig:lossdifference}. The second part of this figure shows the average difference in quadratic loss on the test objects of the supervised and semi-supervised procedures (the inductive setting).

In the transductive setting the loss of the semi-supervised projection solution is lower than that of the supervised case in all of the resamplings taken from the original dataset. Compare this to the behavior of the self-learner. While on average, the performance is quite similar on these datasets, on a particular sample from a dataset, self-learning may lead to a higher quadratic loss than the supervised solution. This result is an illustration of Theorem \ref{th:robustness}. It is favourable to have no deterioration in every resampling because in practice one does not deal with resamplings from an empirical distribution, but rather with a single dataset. A semi-supervised procedure should ideally work on this particular dataset, rather than in expectation over all datasets that one might have observed. %We see a similar negative effect for the difference in hinge loss between the SVM and the TSVM. While better parameter choices may improve the number of experiments with a positive difference, the point of this experiment is to illustrate that while semi-supervised methods may improve performance on average, for a particular sample from a dataset there is no guarantee like Theorem \ref{th:robustness} for the projected estimator.

When looking at the plot of the difference in loss on the test set, we find a similar result to the results in the transductive setting. As the number of unlabeled objects grows, we would expect this to happen as the set of unlabeled objects and test objects become more similar. For most of the datasets $1000$ seem to be enough for the robustness to show up on the test set as well.


\begin{figure*}
\centering
\includegraphics[scale=0.50]{Figure1.pdf}
\caption{Difference in terms of average surrogate loss between the supervised and semi-supervised solutions measured on the labeled and unlabeled instances (transductive setting, top) and measured on a separate test set (inductive setting, bottom). Positive values indicate that the semi-supervised method gives a lower average surrogate loss than its supervised counterpart. For both the projected estimator and self-learning this supervised counterpart is the supervised least squares classifier and loss is in terms of quadratic loss. Note that the figure is zoomed in on the area of interest around a loss difference of $0$ and there are some observations outside of the figure, particularly for the Mammography dataset. The projection method considered here, in the transductive setting, never has higher loss than the supervised procedure, as was proven in Theorem \ref{th:robustness}. While the self-learning may perform as well or better on average on some datasets, the figure illustrates it does not exhibit a similar guarantee for a single resampled dataset from the original datasets.}
\label{fig:lossdifference}
\end{figure*}

\subsection{Learning Curves}
To illustrate the behavior of the procedure with increasing amounts of unlabeled data and to further explore the relationship between the quadratic surrogate loss and classification accuracy we generate learning curves in the following manner. For each of three illustrative datasets (Ionosphere, SPECT and USPS), we randomly sample $2 d$ objects as labeled objects and $1024$ as unlabeled object, where $d$ is the dimensionality of PCA components that retain $99\%$ of the variance for the dataset. The remaining objects are used as a test set. For increasing subsets of the unlabeled data $2,4,8,\dots,1024$ we train the supervised and semi-supervised learners and evaluate their performance on the test objects, in terms of classification accuracy as well as in terms of quadratic loss. We consider both the projection procedure where the distance measure is based on the labeled and the unlabeled data (denoted as Projection) as well as the projected estimator that only uses the labeled data in the distance measure (denoted as ProjectionX). The resampling is repeated $1000$ times and averages and standard errors are reported in Figure \ref{fig:learningcurves}.

The first dataset (Ionosphere) in Figure \ref{fig:learningcurves} is an example where the error of the self-learning procedure starts to increase once we add larger amounts of unlabeled data. In terms of the loss, however, the performance continues to increase. This illustrates that a decrease in the surrogate loss does not necessarily translates into a lower classification error. The projected estimators do not suffer from decrease in performance for larger numbers of unlabeled data in this example. In terms of the loss, however, there seems to be little difference between the three methods.

The second dataset (SPECT) is an example where both the self-learning procedure and the conservative projected estimator are not able to get any improvement out of the data, while the less conservative projection does show some improvement in terms of classification error.

On the USPS dataset the self-learning assumptions do seem to hold and it is able to attain a larger performance improvement as the amount of unlabeled data grows. Both in terms of the error and in terms of the loss, the projected estimators show smaller, but significant improvements.

\begin{figure*}
\centering
\includegraphics[scale=0.50]{Figure2.pdf}
\caption{Learning curves in terms of classification errors and quadratic loss on the test set for increasing numbers of \emph{unlabeled} data on three illustrative datasets. The lines indicate average errors respectively losses on the test set, averaged over $1000$ repeats. The shaded bars indicate standard errors around the mean. The slight increase in the standard error for larger amounts of unlabeled data, which is especially apparent for the supervised curve, is caused by the decreasing size of the test set as one increases the number of objects used as unlabeled training data.}
\label{fig:learningcurves}
\end{figure*}

\subsection{Performance}
In a third experiment, we look at all $23$ datasets and apply a cross-validation procedure to compare the performance in terms of the classification error of several combinations of supervised and semi-supervised classifiers. The cross-validation experiments were set up as follows. For each dataset, the objects were split into 10-folds. Subsequently leaving out each fold, we combine the other 9 folds and randomly select $d+5$ labeled objects while the rest is used as unlabeled objects. We end up with a single prediction for each object, for which we evaluate the misclassification error. This procedure is repeated $10$ times and the averages are reported in Table 1.

The results indicate that in terms of classification errors, the projection procedure never significantly reduces performance over the supervised solution. This is in contrast to the self-learner, which does increase classification error on $5$ of the datasets. The price the projected estimator pays for this robustness, is fewer significant improvements than the less conservative self-learner ($8$ vs. $11$ for the self-learner) and in cases where improvement does occur, it is smaller than that caused by the self-learning procedure.

\begin{table*}[t]
\center
\caption{10-fold 10 repeat Cross-validation results for 23 datasets for the supervised least squares classifier (LS), the projected least squares classifier (Projected) and the self-learned least squares classifier (SL). \textbf{Bold} values indicate whether the performance of a semi-supervised solution is significantly different from the supervised alternative as evaluated by a two-sided paired t-test. \underline{Underlined} values indicate when the performance of the semi-supervised solution is significantly \emph{worse}. The Win/Draw/Loss indicates on how many datasets a semi-supervised learner performs respectively better, equal or worse than the supervised alternative.}
\smallskip
\smallskip
\smallskip
\begin{tabular}{|l|lll|}
\hline
Dataset & Supervised & Self-Learning & Projection \\ 
\hline
Haberman & $0.28 \pm 0.02$& $0.28 \pm 0.02$& $0.28 \pm 0.02$\\ 
Ionosphere & $0.28 \pm 0.02$& $\mathbf{0.24 \pm 0.02} $& $\mathbf{0.23 \pm 0.03} $\\ 
Parkinsons & $0.28 \pm 0.02$& $0.27 \pm 0.03$& $0.27 \pm 0.02$\\ 
Diabetes & $0.32 \pm 0.02$& $\mathbf{\underline{0.35 \pm 0.01}} $& $0.31 \pm 0.02$\\ 
Sonar & $0.40 \pm 0.04$& $\mathbf{0.35 \pm 0.03} $& $\mathbf{0.36 \pm 0.04} $\\ 
SPECT & $0.40 \pm 0.03$& $0.38 \pm 0.03$& $0.39 \pm 0.02$\\ 
SPECTF & $0.42 \pm 0.04$& $0.40 \pm 0.04$& $0.41 \pm 0.03$\\ 
Transfusion & $0.27 \pm 0.02$& $0.27 \pm 0.02$& $0.27 \pm 0.02$\\ 
WDBC & $0.10 \pm 0.01$& $\mathbf{\underline{0.12 \pm 0.02}} $& $0.10 \pm 0.01$\\ 
Mammographic & $0.30 \pm 0.02$& $0.29 \pm 0.02$& $0.30 \pm 0.02$\\ 
Spambase & $0.22 \pm 0.02$& $\mathbf{\underline{0.28 \pm 0.02}} $& $0.22 \pm 0.02$\\ 
Thoraric & $0.27 \pm 0.03$& $\mathbf{0.23 \pm 0.03} $& $0.26 \pm 0.03$\\ 
POP failures & $0.24 \pm 0.04$& $\mathbf{0.16 \pm 0.04} $& $0.23 \pm 0.04$\\ 
EEG eye & $0.46 \pm 0.01$& $\mathbf{\underline{0.48 \pm 0.01}} $& $0.46 \pm 0.01$\\ 
CMC & $0.42 \pm 0.02$& $\mathbf{\underline{0.45 \pm 0.02}} $& $0.42 \pm 0.02$\\ 
Banknote & $0.04 \pm 0.00$& $0.04 \pm 0.00$& $0.04 \pm 0.00$\\ 
Fertility & $0.21 \pm 0.03$& $\mathbf{0.18 \pm 0.03} $& $0.20 \pm 0.03$\\ 
Digit1 & $0.41 \pm 0.02$& $\mathbf{0.33 \pm 0.01} $& $\mathbf{0.37 \pm 0.02} $\\ 
USPS & $0.42 \pm 0.02$& $\mathbf{0.34 \pm 0.02} $& $\mathbf{0.38 \pm 0.02} $\\ 
COIL2 & $0.38 \pm 0.01$& $\mathbf{0.25 \pm 0.01} $& $\mathbf{0.34 \pm 0.01} $\\ 
BCI & $0.41 \pm 0.03$& $\mathbf{0.36 \pm 0.01} $& $\mathbf{0.36 \pm 0.02} $\\ 
g241c & $0.44 \pm 0.02$& $\mathbf{0.39 \pm 0.02} $& $\mathbf{0.41 \pm 0.02} $\\ 
g241d & $0.44 \pm 0.01$& $\mathbf{0.38 \pm 0.01} $& $\mathbf{0.41 \pm 0.01} $\\
\hline
\hline
Win/Draw/Loss & & 11 / 7 / 5 & 8 / 15 / 0 \\ 
\hline
\end{tabular}


\end{table*}

\section{Discussion}
The main result of this work was presented in Theorem \ref{th:robustness} and illustrated in the first experiment: the semi-supervised projected estimator presented here is guaranteed to improve over the supervised estimator in terms of quadratic loss in the transductive setting. The results from the experiment indicate that on average the projected estimator has similarities to self-learning, while on individual samples from the datasets, the projected estimator never reduces performance in terms of the surrogate loss. This is an important property since, in practical settings, one only has a single sample (i.e. dataset) from a classification problem, and it is important to know that performance will not be degraded when applying a semi-supervised version of a supervised procedure. Even if we do not have enough labeled objects to accurately estimate this performance, Theorem \ref{th:robustness} guarantees we will not perform worse than supervised learning in the transductive setting. 



Theorem \ref{th:robustness} is limited to improvement in terms of quadratic loss. As the experiments also indicate, good properties in terms of this loss do not necessarily translate into good properties in terms of the error rate.

In the empirical risk minimization framework, however, classifiers are found by minimizing surrogate losses. In this work, we were able to show that this particular semi-supervised learner is effective at this objective. In this sense, it can perhaps be considered a proper semi-supervised version of the supervised quadratic loss minimizer. Whether and when this increase in performance in terms of the surrogate loss translates into improved classification accuracy is unclear. Much work is currently being done to understand the relationship between these surrogate losses and the ultimate object of many classification problems: $\{0,1\}$-loss \citep{Bartlett2006, Ben-David2012}.

While experiment 2 and 3 indicate that the robustness property also often occurs in terms of classification error, one could question whether the quadratic loss is a good choice as surrogate loss. Its symmetry makes it quite different from $\{0,1\}$-loss that is often of interest in practice. On the other hand, estimation error may be smaller than for other loss functions in the small sample setting. \citep{Ben-David2012} make the case that for linear function classes, this may not off-set the inherent large deviation from $\{0,1\}$-loss. Apart from the fact that their result is derived in a worst case analysis, there are several other reasons why quadratic loss is still interesting and important to consider. Firstly, it performs very well in practice and is often on par or better than, for instance, an SVM employing hinge loss \citep{Rasmussen2005,Hastie2001,Poggio2003}. Moreover, while we considered the classification setting here, the quadratic loss relates closely to least squares regression. While in regression, the constrained region $\Theta$ does not follow directly from the data alone, there may be other ways to construct these regions. For instance, if the outputs of the regression are bounded we have a similar setting to the $[0,1]$ bounds used in the classification scenario considered here.

Arguably, a robust semi-supervised learning procedure could also be derived by very conservatively setting the parameters controlling the influence of unlabeled data in semi-supervised learner procedures such as the TSVM. There are two reasons why this is difficult to achieve in practice. The first reason is a computational one. Most semi-supervised procedures are computationally intensive. Doing a grid search over both a regularization parameter as well as the parameter controlling the influence of the unlabeled objects using cross-validation is time-consuming. Secondly, and perhaps more importantly, it may be very difficult to choose a good parameter using limited labeled data. \citep{Goldberg2009} study this problem in more detail. While their conclusion suggests otherwise, their results indicate that performance degradation occurs on a significant number of datasets. The projected estimators presented here try to alleviate these problem in two ways. Firstly, unlike many semi-supervised procedures, it can be formulated as a quadratic programming problem in terms of the unlabeled objects which has a global optimum (which is unique in terms of $\vec{w}$) and there are no hyper-parameters involved. Secondly, at least in terms of its surrogate loss, there is a guarantee performance will not be worse than the alternative of discarding the unlabeled data.

This begs the question, however, whether we can apply the procedure presented here to other loss functions. Apart from the issue of defining the metric in these cases, some other loss function do not constrain the parameter space in any useful way. For instance, in the case of hinge loss, or logistic loss, one can show that the constrained space $\Theta$ always includes $\vec{w}_\text{sup}$. For these cases, additional constraints may be required to get anything out of the projection method. For the TSVM, for instance, the method would then be similar to that proposed in \citep{Li2011}, although the more general viewpoint of projections considered here may lead to new insights into solving the difficult optimization problem proposed in their work. Many other interesting loss functions, however, do lead to a constrained space that does not include the supervised solution. In particular, many likelihood based classifiers may be amenable to this procedure.

As our results indicate, the proposed procedure is very conservative. Less conservative approaches could be derived by changing the constraint region $\Theta$. The purpose of this work has been to show that if we choose $\Theta$ conservatively, such that we can guarantee it contains the oracle solution $\vec{w}_\text{oracle}$, we can guarantee non-degradation, while still allowing for improved performance over the supervised solution in many cases. The insight that this is possible for any classifier has been the main contribution. To construct a method with more wide applicability, an interesting question is how to restrict $\Theta$ based on additional assumptions, while ensuring that $\vec{w}_\text{oracle} \in \Theta$ with high probability.

\section{Conclusion}
We introduced and analyzed an approach to semi-supervised learning with quadratic surrogate loss that has the interesting theoretical property of never degrading performance in the transductive setting in terms of this surrogate loss. This was achieved by projecting the solution vector of the supervised least squares classifier onto a constrained set of solutions defined by the unlabeled data. As we have illustrated through simulation experiments, the safe improvements in terms of the surrogate loss also partially translates into safe improvements in terms of the classification errors. Moreover, the procedure can be formulated as a standard quadratic programming problem, leading to a simple optimization procedure. An open problem is how to apply this procedure or a procedure with similar theoretical performance guarantees, to other loss functions.

% \section*{Acknowledgements}
% This work was funded by project P23 of the Dutch public-private research community COMMIT.

\bibliography{library}
\bibliographystyle{icml2016}

\end{document} 